{"repo_name":{"2":"bmccann\/examples","3":"intel-analytics\/analytics-zoo","4":"intel-analytics\/analytics-zoo"},"path":{"2":"super_resolution\/main.py","3":"pyzoo\/zoo\/chronos\/model\/tcmf\/DeepGLO.py","4":"pyzoo\/zoo\/automl\/model\/base_pytorch_model.py"},"copies":{"2":6,"3":1,"4":1},"size":{"2":3291,"3":32279,"4":15238},"content":{"2":"from __future__ import print_function\nimport argparse\nfrom math import log10\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom model import Net\nfrom data import get_training_set, get_test_set\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch Super Res Example')\nparser.add_argument('--upscale_factor', type=int, required=True, help=\"super resolution upscale factor\")\nparser.add_argument('--batchSize', type=int, default=64, help='training batch size')\nparser.add_argument('--testBatchSize', type=int, default=10, help='testing batch size')\nparser.add_argument('--nEpochs', type=int, default=2, help='number of epochs to train for')\nparser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')\nparser.add_argument('--cuda', action='store_true', help='use cuda?')\nparser.add_argument('--threads', type=int, default=4, help='number of threads for data loader to use')\nparser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\nopt = parser.parse_args()\n\nprint(opt)\n\ncuda = opt.cuda\nif cuda and not torch.cuda.is_available():\n    raise Exception(\"No GPU found, please run without --cuda\")\n\ntorch.manual_seed(opt.seed)\nif cuda:\n    torch.cuda.manual_seed(opt.seed)\n\nprint('===> Loading datasets')\ntrain_set = get_training_set(opt.upscale_factor)\ntest_set = get_test_set(opt.upscale_factor)\ntraining_data_loader = DataLoader(dataset=train_set, num_workers=opt.threads, batch_size=opt.batchSize, shuffle=True)\ntesting_data_loader = DataLoader(dataset=test_set, num_workers=opt.threads, batch_size=opt.testBatchSize, shuffle=False)\n\nprint('===> Building model')\nmodel = Net(upscale_factor=opt.upscale_factor)\ncriterion = nn.MSELoss()\n\nif cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n\noptimizer = optim.Adam(model.parameters(), lr=opt.lr)\n\n\ndef train(epoch):\n    epoch_loss = 0\n    for iteration, batch in enumerate(training_data_loader, 1):\n        input, target = Variable(batch[0]), Variable(batch[1])\n        if cuda:\n            input = input.cuda()\n            target = target.cuda()\n\n        optimizer.zero_grad()\n        loss = criterion(model(input), target)\n        epoch_loss += loss.data[0]\n        loss.backward()\n        optimizer.step()\n\n        print(\"===> Epoch[{}]({}\/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.data[0]))\n\n    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss \/ len(training_data_loader)))\n\n\ndef test():\n    avg_psnr = 0\n    for batch in testing_data_loader:\n        input, target = Variable(batch[0]), Variable(batch[1])\n        if cuda:\n            input = input.cuda()\n            target = target.cuda()\n\n        prediction = model(input)\n        mse = criterion(prediction, target)\n        psnr = 10 * log10(1 \/ mse.data[0])\n        avg_psnr += psnr\n    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr \/ len(testing_data_loader)))\n\n\ndef checkpoint(epoch):\n    model_out_path = \"model_epoch_{}.pth\".format(epoch)\n    torch.save(model, model_out_path)\n    print(\"Checkpoint saved to {}\".format(model_out_path))\n\nfor epoch in range(1, opt.nEpochs + 1):\n    train(epoch)\n    test()\n    checkpoint(epoch)\n","3":"# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# This file is adapted from the DeepGlo Project. https:\/\/github.com\/rajatsen91\/deepglo\n#\n# Note: This license has also been called the \"New BSD License\" or \"Modified BSD License\". See also\n# the 2-clause BSD License.\n#\n# Copyright (c) 2019 The DeepGLO Project.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification, are permitted\n# provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this list of conditions\n# and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice, this list of\n# conditions and the following disclaimer in the documentation and\/or other materials provided\n# with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its contributors may be used to\n# endorse or promote products derived from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR\n# IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY\n# AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,\n# OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\nfrom __future__ import print_function\nimport torch\nimport numpy as np\nimport pandas as pd\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom zoo.chronos.model.tcmf.data_loader import TCMFDataLoader\nfrom zoo.chronos.model.tcmf.local_model import TemporalConvNet, LocalModel\nfrom zoo.chronos.model.tcmf.time import TimeCovariates\n\nimport copy\n\nimport pickle\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nconsole = logging.StreamHandler()\nlogger.addHandler(console)\n\n\ndef get_model(A, y, lamb=0):\n    \"\"\"\n    Regularized least-squares\n    \"\"\"\n    n_col = A.shape[1]\n    return np.linalg.lstsq(\n        A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y), rcond=None\n    )\n\n\nclass DeepGLO(object):\n    def __init__(\n        self,\n        vbsize=150,\n        hbsize=256,\n        num_channels_X=[32, 32, 32, 32, 1],\n        num_channels_Y=[32, 32, 32, 32, 1],\n        kernel_size=7,\n        dropout=0.2,\n        rank=64,\n        kernel_size_Y=7,\n        lr=0.0005,\n        normalize=False,\n        use_time=True,\n        svd=False,\n        forward_cov=False,\n    ):\n\n        self.use_time = use_time\n        self.dropout = dropout\n        self.forward_cov = forward_cov\n        self.Xseq = TemporalConvNet(\n            num_inputs=1,\n            num_channels=num_channels_X,\n            kernel_size=kernel_size,\n            dropout=dropout,\n            init=True,\n        )\n        self.vbsize = vbsize\n        self.hbsize = hbsize\n        self.num_channels_X = num_channels_X\n        self.num_channels_Y = num_channels_Y\n        self.kernel_size_Y = kernel_size_Y\n        self.rank = rank\n        self.kernel_size = kernel_size\n        self.lr = lr\n        self.normalize = normalize\n        self.svd = svd\n\n    def tensor2d_to_temporal(self, T):\n        T = T.view(1, T.size(0), T.size(1))\n        T = T.transpose(0, 1)\n        return T\n\n    def temporal_to_tensor2d(self, T):\n        T = T.view(T.size(0), T.size(2))\n        return T\n\n    def calculate_newX_loss_vanilla(self, Xn, Fn, Yn, Xf, alpha):\n        Yout = torch.mm(Fn, Xn)\n        cr1 = nn.L1Loss()\n        cr2 = nn.MSELoss()\n        l1 = cr2(Yout, Yn) \/ torch.mean(Yn ** 2)\n        l2 = cr2(Xn, Xf) \/ torch.mean(Xf ** 2)\n        return (1 - alpha) * l1 + alpha * l2\n\n    def recover_future_X(\n        self,\n        last_step,\n        future,\n        num_epochs=50,\n        alpha=0.5,\n        vanilla=True,\n        tol=1e-7,\n    ):\n        rg = max(\n            1 + 2 * (self.kernel_size - 1) * 2 ** (len(self.num_channels_X) - 1),\n            1 + 2 * (self.kernel_size_Y - 1) * 2 ** (len(self.num_channels_Y) - 1),\n        )\n        X = self.X[:, last_step - rg: last_step]\n        X = self.tensor2d_to_temporal(X)\n        outX = self.predict_future(model=self.Xseq, inp=X, future=future)\n        outX = self.temporal_to_tensor2d(outX)\n        Xf = outX[:, -future::]\n        Yn = self.Ymat[:, last_step: last_step + future]\n        Yn = torch.from_numpy(Yn).float()\n\n        Fn = self.F\n\n        Xt = torch.zeros(self.rank, future).float()\n        Xn = torch.normal(Xt, 0.1)\n\n        lprev = 0\n        for i in range(num_epochs):\n            Xn = Variable(Xn, requires_grad=True)\n            optim_Xn = optim.Adam(params=[Xn], lr=self.lr)\n            optim_Xn.zero_grad()\n            loss = self.calculate_newX_loss_vanilla(\n                Xn, Fn.detach(), Yn.detach(), Xf.detach(), alpha\n            )\n            loss.backward()\n            optim_Xn.step()\n            # Xn = torch.clamp(Xn.detach(), min=0)\n\n            if np.abs(lprev - loss.item()) <= tol:\n                break\n\n            if i % 1000 == 0:\n                print(f\"Recovery Loss of epoch {i} is: \" + str(loss.item()))\n                lprev = loss.item()\n\n        return Xn.detach()\n\n    def step_factX_loss(self, inp, out, last_vindex, last_hindex, reg=0.0):\n        Xout = self.X[:, last_hindex + 1: last_hindex + 1 + out.size(2)]\n        Fout = self.F[self.D.I[last_vindex: last_vindex + out.size(0)], :]\n        Xout = Variable(Xout, requires_grad=True)\n        out = self.temporal_to_tensor2d(out)\n        optim_X = optim.Adam(params=[Xout], lr=self.lr)\n        Hout = torch.matmul(Fout, Xout)\n        optim_X.zero_grad()\n        loss = torch.mean(torch.pow(Hout - out.detach(), 2))\n        l2 = torch.mean(torch.pow(Xout, 2))\n        r = loss.detach() \/ l2.detach()\n        loss = loss + r * reg * l2\n        loss.backward()\n        optim_X.step()\n        # Xout = torch.clamp(Xout, min=0)\n        self.X[:, last_hindex + 1: last_hindex + 1 + inp.size(2)] = Xout.detach()\n        return loss\n\n    def step_factF_loss(self, inp, out, last_vindex, last_hindex, reg=0.0):\n        Xout = self.X[:, last_hindex + 1: last_hindex + 1 + out.size(2)]\n        Fout = self.F[self.D.I[last_vindex: last_vindex + out.size(0)], :]\n        Fout = Variable(Fout, requires_grad=True)\n        optim_F = optim.Adam(params=[Fout], lr=self.lr)\n        out = self.temporal_to_tensor2d(out)\n        Hout = torch.matmul(Fout, Xout)\n        optim_F.zero_grad()\n        loss = torch.mean(torch.pow(Hout - out.detach(), 2))\n        l2 = torch.mean(torch.pow(Fout, 2))\n        r = loss.detach() \/ l2.detach()\n        loss = loss + r * reg * l2\n        loss.backward()\n        optim_F.step()\n        self.F[\n            self.D.I[last_vindex: last_vindex + inp.size(0)], :\n        ] = Fout.detach()\n        return loss\n\n    def step_temporal_loss_X(self, inp, last_vindex, last_hindex):\n        Xin = self.X[:, last_hindex: last_hindex + inp.size(2)]\n        Xout = self.X[:, last_hindex + 1: last_hindex + 1 + inp.size(2)]\n        for p in self.Xseq.parameters():\n            p.requires_grad = False\n        Xin = Variable(Xin, requires_grad=True)\n        Xout = Variable(Xout, requires_grad=True)\n        optim_out = optim.Adam(params=[Xout], lr=self.lr)\n        Xin = self.tensor2d_to_temporal(Xin)\n        Xout = self.tensor2d_to_temporal(Xout)\n        hatX = self.Xseq(Xin)\n        optim_out.zero_grad()\n        loss = torch.mean(torch.pow(Xout - hatX.detach(), 2))\n        loss.backward()\n        optim_out.step()\n        # Xout = torch.clamp(Xout, min=0)\n        temp = self.temporal_to_tensor2d(Xout.detach())\n        self.X[:, last_hindex + 1: last_hindex + 1 + inp.size(2)] = temp\n        return loss\n\n    def predict_future_batch(self, model, inp, future=10):\n        out = model(inp)\n        output = out[:, :, out.size(2) - 1].view(out.size(0), out.size(1), 1)\n        out = torch.cat((inp, output), dim=2)\n        for i in range(future - 1):\n            inp = out\n            out = model(inp)\n            output = out[:, :, out.size(2) - 1].view(out.size(0), out.size(1), 1)\n            out = torch.cat((inp, output), dim=2)\n\n        out = self.temporal_to_tensor2d(out)\n        out = np.array(out.detach())\n        return out\n\n    def predict_future(self, model, inp, future=10, bsize=90):\n        n = inp.size(0)\n        ids = np.arange(0, n, bsize)\n        ids = list(ids) + [n]\n        out = self.predict_future_batch(model, inp[ids[0]: ids[1], :, :], future)\n\n        for i in range(1, len(ids) - 1):\n            temp = self.predict_future_batch(\n                model, inp[ids[i]: ids[i + 1], :, :], future\n            )\n            out = np.vstack([out, temp])\n\n        out = torch.from_numpy(out).float()\n        return self.tensor2d_to_temporal(out)\n\n    def predict_global(\n        self, ind, last_step=100, future=10, normalize=False, bsize=90\n    ):\n\n        if ind is None:\n            ind = np.arange(self.Ymat.shape[0])\n\n        self.Xseq = self.Xseq.eval()\n\n        rg = max(\n            1 + 2 * (self.kernel_size - 1) * 2 ** (len(self.num_channels_X) - 1),\n            1 + 2 * (self.kernel_size_Y - 1) * 2 ** (len(self.num_channels_Y) - 1),\n        )\n        X = self.X[:, last_step - rg: last_step]\n        n = X.size(0)\n        T = X.size(1)\n        X = self.tensor2d_to_temporal(X)\n        outX = self.predict_future(\n            model=self.Xseq, inp=X, future=future, bsize=bsize\n        )\n\n        outX = self.temporal_to_tensor2d(outX)\n\n        F = self.F\n\n        Y = torch.matmul(F, outX)\n\n        Y = np.array(Y[ind, :].detach())\n\n        del F\n\n        for p in self.Xseq.parameters():\n            p.requires_grad = True\n\n        if normalize:\n            Y = Y - self.mini\n            Y = Y * self.s[ind, None] + self.m[ind, None]\n            return Y\n        else:\n            return Y\n\n    def train_Xseq(self, Ymat, num_epochs=20, val_len=24, early_stop=False, tenacity=3):\n        seq = self.Xseq\n        num_channels = self.num_channels_X\n        kernel_size = self.kernel_size\n        vbsize = min(self.vbsize, Ymat.shape[0] \/ 2)\n\n        for p in seq.parameters():\n            p.requires_grad = True\n\n        TC = LocalModel(\n            Ymat=Ymat,\n            num_inputs=1,\n            num_channels=num_channels,\n            kernel_size=kernel_size,\n            vbsize=vbsize,\n            hbsize=self.hbsize,\n            normalize=False,\n            end_index=self.end_index - val_len,\n            val_len=val_len,\n            lr=self.lr,\n        )\n\n        TC.train_model(num_epochs=num_epochs, early_stop=early_stop, tenacity=tenacity)\n\n        self.Xseq = TC.seq\n\n    def train_factors(\n        self,\n        reg_X=0.0,\n        reg_F=0.0,\n        mod=5,\n        val_len=24,\n        early_stop=False,\n        tenacity=3,\n        ind=None,\n        seed=False,\n    ):\n        self.D.epoch = 0\n        self.D.vindex = 0\n        self.D.hindex = 0\n        for p in self.Xseq.parameters():\n            p.requires_grad = True\n\n        l_F = [0.0]\n        l_X = [0.0]\n        l_X_temporal = [0.0]\n        iter_count = 0\n        vae = float(\"inf\")\n        scount = 0\n        Xbest = self.X.clone()\n        Fbest = self.F.clone()\n        while self.D.epoch < self.num_epochs:\n            last_epoch = self.D.epoch\n            last_vindex = self.D.vindex\n            last_hindex = self.D.hindex\n            inp, out, vindex, hindex = self.D.next_batch()\n\n            step_l_F = self.step_factF_loss(inp, out, last_vindex, last_hindex, reg=reg_F)\n            l_F = l_F + [step_l_F.item()]\n            step_l_X = self.step_factX_loss(inp, out, last_vindex, last_hindex, reg=reg_X)\n            l_X = l_X + [step_l_X.item()]\n            if seed is False and iter_count % mod == 1:\n                l2 = self.step_temporal_loss_X(inp, last_vindex, last_hindex)\n                l_X_temporal = l_X_temporal + [l2.item()]\n            iter_count = iter_count + 1\n\n            if self.D.epoch > last_epoch:\n                print(\"Entering Epoch#{}\".format(self.D.epoch))\n                print(\"Factorization Loss F:{}\".format(np.mean(l_F)))\n                print(\"Factorization Loss X:{}\".format(np.mean(l_X)))\n                print(\"Temporal Loss X:{}\".format(np.mean(l_X_temporal)))\n\n                if ind is None:\n                    ind = np.arange(self.Ymat.shape[0])\n                else:\n                    ind = ind\n                inp = self.predict_global(\n                    ind,\n                    last_step=self.end_index - val_len,\n                    future=val_len,\n                )\n                R = self.Ymat[ind, self.end_index - val_len: self.end_index]\n                S = inp[:, -val_len::]\n                ve = np.abs(R - S).mean() \/ np.abs(R).mean()\n                # print(\"Validation Loss (Global): \", ve)\n                print(\"Validation Loss (Global):{}\".format(ve))\n                if ve <= vae:\n                    vae = ve\n                    scount = 0\n                    Xbest = self.X.clone()\n                    Fbest = self.F.clone()\n                    # Xseqbest = TemporalConvNet(\n                    #     num_inputs=1,\n                    #     num_channels=self.num_channels_X,\n                    #     kernel_size=self.kernel_size,\n                    #     dropout=self.dropout,\n                    # )\n                    # Xseqbest.load_state_dict(self.Xseq.state_dict())\n                    Xseqbest = pickle.loads(pickle.dumps(self.Xseq))\n                else:\n                    scount += 1\n                    if scount > tenacity and early_stop:\n                        # print(\"Early Stopped\")\n                        print(\"Early Stopped\")\n                        self.X = Xbest\n                        self.F = Fbest\n                        self.Xseq = Xseqbest\n                        break\n\n    def create_Ycov(self):\n        t0 = self.end_index + 1\n        self.D.epoch = 0\n        self.D.vindex = 0\n        self.D.hindex = 0\n        Ycov = copy.deepcopy(self.Ymat[:, 0:t0])\n        Ymat_now = self.Ymat[:, 0:t0]\n\n        self.Xseq = self.Xseq.eval()\n\n        while self.D.epoch < 1:\n            last_epoch = self.D.epoch\n            last_vindex = self.D.vindex\n            last_hindex = self.D.hindex\n            inp, out, vindex, hindex = self.D.next_batch()\n\n            Xin = self.tensor2d_to_temporal(self.X[:, last_hindex: last_hindex + inp.size(2)])\n            Xout = self.temporal_to_tensor2d(self.Xseq(Xin))\n            Fout = self.F[self.D.I[last_vindex: last_vindex + out.size(0)], :]\n            output = np.array(torch.matmul(Fout, Xout).detach())\n            Ycov[\n                last_vindex: last_vindex + output.shape[0],\n                last_hindex + 1: last_hindex + 1 + output.shape[1],\n            ] = output\n\n        for p in self.Xseq.parameters():\n            p.requires_grad = True\n\n        if self.period is None:\n            Ycov_wc = np.zeros(shape=[Ycov.shape[0], 1, Ycov.shape[1]])\n            if self.forward_cov:\n                Ycov_wc[:, 0, 0:-1] = Ycov[:, 1::]\n            else:\n                Ycov_wc[:, 0, :] = Ycov\n        else:\n            Ycov_wc = np.zeros(shape=[Ycov.shape[0], 2, Ycov.shape[1]])\n            if self.forward_cov:\n                Ycov_wc[:, 0, 0:-1] = Ycov[:, 1::]\n            else:\n                Ycov_wc[:, 0, :] = Ycov\n            Ycov_wc[:, 1, self.period - 1::] = Ymat_now[:, 0: -(self.period - 1)]\n        return Ycov_wc\n\n    def train_Yseq(self, num_epochs=20,\n                   covariates=None,\n                   dti=None,\n                   val_len=24,\n                   num_workers=1,\n                   ):\n        Ycov = self.create_Ycov()\n        self.Yseq = LocalModel(\n            self.Ymat,\n            num_inputs=1,\n            num_channels=self.num_channels_Y,\n            kernel_size=self.kernel_size_Y,\n            dropout=self.dropout,\n            vbsize=self.vbsize,\n            hbsize=self.hbsize,\n            lr=self.lr,\n            val_len=val_len,\n            test=True,\n            end_index=self.end_index - val_len,\n            normalize=False,\n            start_date=self.start_date,\n            freq=self.freq,\n            covariates=covariates,\n            use_time=self.use_time,\n            dti=dti,\n            Ycov=Ycov,\n        )\n        val_loss = self.Yseq.train_model(num_epochs=num_epochs,\n                                         num_workers=num_workers,\n                                         early_stop=False)\n        return val_loss\n\n    def train_all_models(\n            self,\n            Ymat,\n            val_len=24,\n            start_date=\"2016-1-1\",\n            freq=\"H\",\n            covariates=None,\n            dti=None,\n            period=None,\n            init_epochs=100,\n            alt_iters=10,\n            y_iters=200,\n            tenacity=7,\n            mod=5,\n            max_FX_epoch=300,\n            max_TCN_epoch=300,\n            num_workers=1,\n    ):\n        self.end_index = Ymat.shape[1]\n        self.start_date = start_date\n        self.freq = freq\n        self.period = period\n        self.covariates = covariates\n        self.dti = dti\n\n        if self.normalize:\n            self.s = np.std(Ymat[:, 0:self.end_index], axis=1)\n            # self.s[self.s == 0] = 1.0\n            self.s += 1.0\n            self.m = np.mean(Ymat[:, 0:self.end_index], axis=1)\n            self.Ymat = (Ymat - self.m[:, None]) \/ self.s[:, None]\n            self.mini = np.abs(np.min(self.Ymat))\n            self.Ymat = self.Ymat + self.mini\n        else:\n            self.Ymat = Ymat\n\n        n, T = self.Ymat.shape\n        t0 = self.end_index + 1\n        if t0 > T:\n            self.Ymat = np.hstack([self.Ymat, self.Ymat[:, -1].reshape(-1, 1)])\n        if self.svd:\n            indices = np.random.choice(self.Ymat.shape[0], self.rank, replace=False)\n            X = self.Ymat[indices, 0:t0]\n            mX = np.std(X, axis=1)\n            mX[mX == 0] = 1.0\n            X = X \/ mX[:, None]\n            Ft = get_model(X.transpose(), self.Ymat[:, 0:t0].transpose(), lamb=0.1)\n            F = Ft[0].transpose()\n            self.X = torch.from_numpy(X).float()\n            self.F = torch.from_numpy(F).float()\n        else:\n            R = torch.zeros(self.rank, t0).float()\n            X = torch.normal(R, 0.1)\n            C = torch.zeros(n, self.rank).float()\n            F = torch.normal(C, 0.1)\n            self.X = X.float()\n            self.F = F.float()\n\n        self.D = TCMFDataLoader(\n            Ymat=self.Ymat,\n            vbsize=self.vbsize,\n            hbsize=self.hbsize,\n            end_index=self.end_index,\n            val_len=val_len,\n            shuffle=False,\n        )\n        # print(\"-\"*50+\"Initializing Factors.....\")\n        logger.info(\"Initializing Factors\")\n        self.num_epochs = init_epochs\n        self.train_factors(val_len=val_len)\n\n        if alt_iters % 2 == 1:\n            alt_iters += 1\n\n        # print(\"Starting Alternate Training.....\")\n        logger.info(\"Starting Alternate Training.....\")\n\n        for i in range(1, alt_iters):\n            if i % 2 == 0:\n                logger.info(\"Training Factors. Iter#:{}\".format(i))\n                self.num_epochs = max_FX_epoch\n                self.train_factors(\n                    seed=False, val_len=val_len,\n                    early_stop=True, tenacity=tenacity, mod=mod\n                )\n            else:\n                # logger.info(\n                #     \"--------------------------------------------Training Xseq Model. Iter#:{}\"\n                #     .format(i)\n                #     + \"-------------------------------------------------------\"\n                # )\n                logger.info(\"Training Xseq Model. Iter#:{}\".format(i))\n\n                self.num_epochs = max_TCN_epoch\n                T = np.array(self.X.detach())\n                self.train_Xseq(\n                    Ymat=T,\n                    num_epochs=self.num_epochs,\n                    val_len=val_len,\n                    early_stop=True,\n                    tenacity=tenacity,\n                )\n\n        logger.info(\"Start training Yseq.....\")\n        val_loss = self.train_Yseq(num_epochs=y_iters,\n                                   covariates=covariates,\n                                   dti=dti,\n                                   val_len=val_len,\n                                   num_workers=num_workers,\n                                   )\n        return val_loss\n\n    def append_new_y(self, Ymat_new, covariates_new=None, dti_new=None):\n        # update Yseq\n        # normalize the incremented Ymat if needed\n        if self.normalize:\n            Ymat_new = (Ymat_new - self.m[:, None]) \/ self.s[:, None]\n            Ymat_new = Ymat_new + self.mini\n\n        # append the new Ymat onto the original, note that self.end_index equals to the no.of time\n        # steps of the original.\n        n, T_added = Ymat_new.shape\n        self.Ymat = np.concatenate((self.Ymat[:, : self.end_index], Ymat_new), axis=1)\n        self.end_index = self.end_index + T_added\n\n        n, T = self.Ymat.shape\n        t0 = self.end_index + 1\n        if t0 > T:\n            self.Ymat = np.hstack([self.Ymat, self.Ymat[:, -1].reshape(-1, 1)])\n\n        # update Yseq.covariates\n        last_step = self.end_index - T_added\n        new_covariates = self.get_future_time_covs(T_added, last_step,\n                                                   future_covariates=covariates_new,\n                                                   future_dti=dti_new)\n        self.Yseq.covariates = np.hstack([self.Yseq.covariates[:, :last_step], new_covariates])\n\n    def inject_new(self,\n                   Ymat_new,\n                   covariates_new=None,\n                   dti_new=None):\n        if self.Ymat.shape[0] != Ymat_new.shape[0]:\n            raise ValueError(\"Expected incremental input with {} time series, got {} instead.\"\n                             .format(self.Ymat.shape[0], Ymat_new.shape[0]))\n        self.append_new_y(Ymat_new, covariates_new=covariates_new, dti_new=dti_new)\n        n, T = self.Ymat.shape\n        rank, XT = self.X.shape\n        future = T - XT\n        Xn = self.recover_future_X(\n            last_step=XT,\n            future=future,\n            num_epochs=100000,\n            alpha=0.3,\n            vanilla=True,\n        )\n        self.X = torch.cat([self.X, Xn], dim=1)\n\n    def get_time_covs(self, future_start_date, num_ts, future_covariates, future_dti):\n        if self.use_time:\n            future_time = TimeCovariates(\n                start_date=future_start_date,\n                freq=self.freq,\n                normalized=True,\n                num_ts=num_ts\n            )\n            if future_dti is not None:\n                future_time.dti = future_dti\n            time_covariates = future_time.get_covariates()\n            if future_covariates is None:\n                covariates = time_covariates\n            else:\n                covariates = np.vstack([time_covariates, future_covariates])\n        else:\n            covariates = future_covariates\n        return covariates\n\n    def get_future_time_covs(self, horizon, last_step, future_covariates, future_dti):\n        if self.freq[0].isalpha():\n            freq = \"1\" + self.freq\n        else:\n            freq = self.freq\n        future_start_date = pd.Timestamp(self.start_date) + pd.Timedelta(freq) * last_step\n        covs_future = self.get_time_covs(future_start_date=future_start_date,\n                                         num_ts=horizon,\n                                         future_covariates=future_covariates,\n                                         future_dti=future_dti)\n        return covs_future\n\n    def get_prediction_time_covs(self, rg, horizon, last_step, future_covariates, future_dti):\n        covs_past = self.Yseq.covariates[:, last_step - rg: last_step]\n        covs_future = self.get_future_time_covs(horizon, last_step, future_covariates, future_dti)\n        covs = np.concatenate([covs_past, covs_future], axis=1)\n        return covs\n\n    def predict_horizon(\n            self,\n            ind=None,\n            future=10,\n            future_covariates=None,\n            future_dti=None,\n            bsize=90,\n            num_workers=1,\n    ):\n        last_step = self.end_index\n        if ind is None:\n            ind = np.arange(self.Ymat.shape[0])\n\n        self.Yseq.seq = self.Yseq.seq.eval()\n        self.Xseq = self.Xseq.eval()\n\n        rg = max(\n            1 + 2 * (self.kernel_size - 1) * 2 ** (len(self.num_channels_X) - 1),\n            1 + 2 * (self.kernel_size_Y - 1) * 2 ** (len(self.num_channels_Y) - 1),\n        )\n        covs = self.get_prediction_time_covs(rg, future, last_step, future_covariates, future_dti)\n\n        yc = self.predict_global(\n            ind=ind,\n            last_step=last_step,\n            future=future,\n            normalize=False,\n            bsize=bsize,\n        )\n\n        if self.period is None:\n            ycovs = np.zeros(shape=[yc.shape[0], 1, yc.shape[1]])\n            if self.forward_cov:\n                ycovs[:, 0, 0:-1] = yc[:, 1::]\n            else:\n                ycovs[:, 0, :] = yc\n        else:\n            ycovs = np.zeros(shape=[yc.shape[0], 2, yc.shape[1]])\n            if self.forward_cov:\n                ycovs[:, 0, 0:-1] = yc[:, 1::]\n            else:\n                ycovs[:, 0, :] = yc\n            period = self.period\n            while last_step + future - (period - 1) > last_step + 1:\n                period += self.period\n            # The last coordinate is not used.\n            ycovs[:, 1, period - 1::] = self.Ymat[\n                :, last_step - rg: last_step + future - (period - 1)]\n\n        Y = self.Yseq.predict_future(\n            data_in=self.Ymat[ind, last_step - rg: last_step],\n            covariates=covs,\n            ycovs=ycovs,\n            future=future,\n            bsize=bsize,\n            normalize=False,\n            num_workers=num_workers,\n        )\n\n        if self.normalize:\n            Y = Y - self.mini\n            Y = Y * self.s[ind, None] + self.m[ind, None]\n            return Y\n        else:\n            return Y\n\n    def predict(\n        self, ind=None, last_step=100, future=10, normalize=False, bsize=90\n    ):\n\n        if ind is None:\n            ind = np.arange(self.Ymat.shape[0])\n\n        self.Xseq = self.Xseq\n\n        self.Yseq.seq = self.Yseq.seq.eval()\n        self.Xseq = self.Xseq.eval()\n\n        rg = max(\n            1 + 2 * (self.kernel_size - 1) * 2 ** (len(self.num_channels_X) - 1),\n            1 + 2 * (self.kernel_size_Y - 1) * 2 ** (len(self.num_channels_Y) - 1),\n        )\n        covs = self.Yseq.covariates[:, last_step - rg: last_step + future]\n        # print(covs.shape)\n        yc = self.predict_global(\n            ind=ind,\n            last_step=last_step,\n            future=future,\n            normalize=False,\n            bsize=bsize,\n        )\n        if self.period is None:\n            ycovs = np.zeros(shape=[yc.shape[0], 1, yc.shape[1]])\n            if self.forward_cov:\n                ycovs[:, 0, 0:-1] = yc[:, 1::]\n            else:\n                ycovs[:, 0, :] = yc\n        else:\n            ycovs = np.zeros(shape=[yc.shape[0], 2, yc.shape[1]])\n            if self.forward_cov:\n                ycovs[:, 0, 0:-1] = yc[:, 1::]\n            else:\n                ycovs[:, 0, :] = yc\n            period = self.period\n            while last_step + future - (period - 1) > last_step + 1:\n                period += self.period\n            # this seems like we are looking ahead, but it will not use the last coordinate,\n            # which is the only new point added\n            ycovs[:, 1, period - 1::] = self.Ymat[\n                :, last_step - rg: last_step + future - (period - 1)]\n\n        Y = self.Yseq.predict_future(\n            data_in=self.Ymat[ind, last_step - rg: last_step],\n            covariates=covs,\n            ycovs=ycovs,\n            future=future,\n            bsize=bsize,\n            normalize=False,\n        )\n\n        if normalize:\n            Y = Y - self.mini\n            Y = Y * self.s[ind, None] + self.m[ind, None]\n            return Y\n        else:\n            return Y\n\n    def rolling_validation(self, Ymat, tau=24, n=7, bsize=90, alpha=0.3):\n        prevX = self.X.clone()\n        prev_index = self.end_index\n        out = self.predict(\n            last_step=self.end_index,\n            future=tau,\n            bsize=bsize,\n            normalize=self.normalize,\n        )\n        out_global = self.predict_global(\n            np.arange(self.Ymat.shape[0]),\n            last_step=self.end_index,\n            future=tau,\n            normalize=self.normalize,\n            bsize=bsize,\n        )\n        predicted_values = []\n        actual_values = []\n        predicted_values_global = []\n        S = out[:, -tau::]\n        S_g = out_global[:, -tau::]\n        predicted_values += [S]\n        predicted_values_global += [S_g]\n        R = Ymat[:, self.end_index: self.end_index + tau]\n        actual_values += [R]\n        print(\"Current window wape:{}\".format(wape(S, R)))\n\n        self.Xseq = self.Xseq.eval()\n        self.Yseq.seq = self.Yseq.seq.eval()\n\n        for i in range(n - 1):\n            Xn = self.recover_future_X(\n                last_step=self.end_index + 1,\n                future=tau,\n                num_epochs=100000,\n                alpha=alpha,\n                vanilla=True\n            )\n            self.X = torch.cat([self.X, Xn], dim=1)\n            self.end_index += tau\n            out = self.predict(\n                last_step=self.end_index,\n                future=tau,\n                bsize=bsize,\n                normalize=self.normalize,\n            )\n            out_global = self.predict_global(\n                np.arange(self.Ymat.shape[0]),\n                last_step=self.end_index,\n                future=tau,\n                normalize=self.normalize,\n                bsize=bsize,\n            )\n            S = out[:, -tau::]\n            S_g = out_global[:, -tau::]\n            predicted_values += [S]\n            predicted_values_global += [S_g]\n            R = Ymat[:, self.end_index: self.end_index + tau]\n            actual_values += [R]\n            print(\"Current window wape:{}\".format(wape(S, R)))\n\n        predicted = np.hstack(predicted_values)\n        predicted_global = np.hstack(predicted_values_global)\n        actual = np.hstack(actual_values)\n\n        dic = {}\n        dic[\"wape\"] = wape(predicted, actual)\n        dic[\"mape\"] = mape(predicted, actual)\n        dic[\"smape\"] = smape(predicted, actual)\n        dic[\"mae\"] = np.abs(predicted - actual).mean()\n        dic[\"rmse\"] = np.sqrt(((predicted - actual) ** 2).mean())\n        dic[\"nrmse\"] = dic[\"rmse\"] \/ np.sqrt(((actual) ** 2).mean())\n\n        dic[\"wape_global\"] = wape(predicted_global, actual)\n        dic[\"mape_global\"] = mape(predicted_global, actual)\n        dic[\"smape_global\"] = smape(predicted_global, actual)\n        dic[\"mae_global\"] = np.abs(predicted_global - actual).mean()\n        dic[\"rmse_global\"] = np.sqrt(((predicted_global - actual) ** 2).mean())\n        dic[\"nrmse_global\"] = dic[\"rmse\"] \/ np.sqrt(((actual) ** 2).mean())\n\n        baseline = Ymat[:, Ymat.shape[1] - n * tau - tau: Ymat.shape[1] - tau]\n        dic[\"baseline_wape\"] = wape(baseline, actual)\n        dic[\"baseline_mape\"] = mape(baseline, actual)\n        dic[\"baseline_smape\"] = smape(baseline, actual)\n        self.X = prevX\n        self.end_index = prev_index\n\n        return dic\n","4":"#\n# Copyright 2018 Analytics Zoo Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport types\n\nfrom zoo.automl.model.abstract import BaseModel, ModelBuilder\nfrom zoo.automl.common.util import *\nfrom zoo.automl.common.metrics import Evaluator\nimport pandas as pd\n\nfrom zoo.orca.automl.pytorch_utils import LR_NAME, DEFAULT_LR\n\nPYTORCH_REGRESSION_LOSS_MAP = {\"mse\": \"MSELoss\",\n                               \"mae\": \"L1Loss\",\n                               \"huber_loss\": \"SmoothL1Loss\"}\n\n\nclass PytorchBaseModel(BaseModel):\n    def __init__(self, model_creator, optimizer_creator, loss_creator,\n                 check_optional_config=False):\n        self.check_optional_config = check_optional_config\n        self.model_creator = model_creator\n        self.optimizer_creator = optimizer_creator\n        self.loss_creator = loss_creator\n        self.config = None\n        self.model = None\n        self.model_built = False\n        self.onnx_model = None\n        self.onnx_model_built = False\n\n    def _create_loss(self):\n        if isinstance(self.loss_creator, torch.nn.modules.loss._Loss):\n            self.criterion = self.loss_creator\n        else:\n            self.criterion = self.loss_creator(self.config)\n\n    def _create_optimizer(self):\n        import types\n        if isinstance(self.optimizer_creator, types.FunctionType):\n            self.optimizer = self.optimizer_creator(self.model, self.config)\n        else:\n            # use torch default parameter values if user pass optimizer name or optimizer class.\n            try:\n                self.optimizer = self.optimizer_creator(self.model.parameters(),\n                                                        lr=self.config.get(LR_NAME, DEFAULT_LR))\n            except:\n                raise ValueError(\"We failed to generate an optimizer with specified optim \"\n                                 \"class\/name. You need to pass an optimizer creator function.\")\n\n    def build(self, config):\n        # check config and update\n        self._check_config(**config)\n        self.config = config\n        # build model\n        if \"selected_features\" in config:\n            config[\"input_feature_num\"] = len(config['selected_features'])\\\n                + config['output_feature_num']\n        self.model = self.model_creator(config)\n        if not isinstance(self.model, torch.nn.Module):\n            raise ValueError(\"You must create a torch model in model_creator\")\n        self.model_built = True\n        self._create_loss()\n        self._create_optimizer()\n\n    def _reshape_input(self, x):\n        if x.ndim == 1:\n            x = x.reshape(-1, 1)\n        return x\n\n    def _np_to_creator(self, data):\n        def data_creator(config):\n                x, y = PytorchBaseModel.covert_input(data)\n                x = self._reshape_input(x)\n                y = self._reshape_input(y)\n                return DataLoader(TensorDataset(x, y),\n                                  batch_size=int(config[\"batch_size\"]),\n                                  shuffle=True)\n        return data_creator\n\n    def fit_eval(self, data, validation_data=None, mc=False, verbose=0, epochs=1, metric=None,\n                 metric_func=None,\n                 **config):\n        \"\"\"\n        :param data: data could be a tuple with numpy ndarray with form (x, y) or a\n               data creator which takes a config dict and returns a\n               torch.utils.data.DataLoader. torch.Tensor should be generated from the\n               dataloader.\n        :param validation_data: validation data could be a tuple with numpy ndarray\n               with form (x, y) or a data creator which takes a config dict and returns a\n               torch.utils.data.DataLoader. torch.Tensor should be generated from the\n               dataloader.\n        fit_eval will build a model at the first time it is built\n        config will be updated for the second or later times with only non-model-arch\n        params be functional\n        TODO: check the updated params and decide if the model is needed to be rebuilt\n        \"\"\"\n        # todo: support input validation data None\n        assert validation_data is not None, \"You must input validation data!\"\n\n        if not metric:\n            raise ValueError(\"You must input a valid metric value for fit_eval.\")\n\n        # update config settings\n        def update_config():\n            if not isinstance(data, types.FunctionType):\n                x = self._reshape_input(data[0])\n                y = self._reshape_input(data[1])\n                config.setdefault(\"past_seq_len\", x.shape[-2])\n                config.setdefault(\"future_seq_len\", y.shape[-2])\n                config.setdefault(\"input_feature_num\", x.shape[-1])\n                config.setdefault(\"output_feature_num\", y.shape[-1])\n\n        if not self.model_built:\n            update_config()\n            self.build(config)\n        else:\n            tmp_config = self.config.copy()\n            tmp_config.update(config)\n            self._check_config(**tmp_config)\n            self.config.update(config)\n\n        # get train_loader and validation_loader\n        if isinstance(data, types.FunctionType):\n            train_loader = data(self.config)\n            validation_loader = validation_data(self.config)\n        else:\n            assert isinstance(data, tuple) and isinstance(validation_data, tuple),\\\n                f\"data\/validation_data should be a tuple or\\\n                 data creator function but found {type(data)}\"\n            assert isinstance(data[0], np.ndarray) and isinstance(validation_data[0], np.ndarray),\\\n                f\"x should be a np.ndarray but found {type(x)}\"\n            assert isinstance(data[1], np.ndarray) and isinstance(validation_data[1], np.ndarray),\\\n                f\"y should be a np.ndarray but found {type(y)}\"\n            train_data_creator = self._np_to_creator(data)\n            valid_data_creator = self._np_to_creator(validation_data)\n            train_loader = train_data_creator(self.config)\n            validation_loader = valid_data_creator(self.config)\n\n        epoch_losses = []\n        for i in range(epochs):\n            train_loss = self._train_epoch(train_loader)\n            epoch_losses.append(train_loss)\n        train_stats = {\"loss\": np.mean(epoch_losses), \"last_loss\": epoch_losses[-1]}\n        val_stats = self._validate(validation_loader, metric_name=metric, metric_func=metric_func)\n        self.onnx_model_built = False\n        return val_stats\n\n    @staticmethod\n    def to_torch(inp):\n        if isinstance(inp, np.ndarray):\n            return torch.from_numpy(inp)\n        if isinstance(inp, (pd.DataFrame, pd.Series)):\n            return torch.from_numpy(inp.values)\n        return inp\n\n    @staticmethod\n    def covert_input(data):\n        x = PytorchBaseModel.to_torch(data[0]).float()\n        y = PytorchBaseModel.to_torch(data[1]).float()\n        return x, y\n\n    def _train_epoch(self, train_loader):\n        self.model.train()\n        total_loss = 0\n        batch_idx = 0\n        tqdm = None\n        try:\n            from tqdm import tqdm\n            pbar = tqdm(total=len(train_loader))\n        except ImportError:\n            pass\n        for x_batch, y_batch in train_loader:\n            self.optimizer.zero_grad()\n            yhat = self._forward(x_batch, y_batch)\n            loss = self.criterion(yhat, y_batch)\n            loss.backward()\n            self.optimizer.step()\n            total_loss += loss.item()\n            batch_idx += 1\n            if tqdm:\n                pbar.set_description(\"Loss: {}\".format(loss.item()))\n                pbar.update(1)\n        if tqdm:\n            pbar.close()\n        train_loss = total_loss\/batch_idx\n        return train_loss\n\n    def _forward(self, x, y):\n        return self.model(x)\n\n    def _validate(self, validation_loader, metric_name, metric_func=None):\n        if not metric_name:\n            assert metric_func, \"You must input valid metric_func or metric_name\"\n            metric_name = metric_func.__name__\n        self.model.eval()\n        with torch.no_grad():\n            yhat_list = []\n            y_list = []\n            for x_valid_batch, y_valid_batch in validation_loader:\n                yhat_list.append(self.model(x_valid_batch).numpy())\n                y_list.append(y_valid_batch.numpy())\n            yhat = np.concatenate(yhat_list, axis=0)\n            y = np.concatenate(y_list, axis=0)\n        # val_loss = self.criterion(yhat, y)\n        if metric_func:\n            eval_result = metric_func(y, yhat)\n        else:\n            eval_result = Evaluator.evaluate(metric=metric_name,\n                                             y_true=y, y_pred=yhat,\n                                             multioutput='uniform_average')\n        return {metric_name: eval_result}\n\n    def _print_model(self):\n        # print model and parameters\n        print(self.model)\n        print(len(list(self.model.parameters())))\n        for i in range(len(list(self.model.parameters()))):\n            print(list(self.model.parameters())[i].size())\n\n    def evaluate(self, x, y, metrics=['mse'], multioutput=\"raw_values\"):\n        # reshape 1dim input\n        x = self._reshape_input(x)\n        y = self._reshape_input(y)\n\n        yhat = self.predict(x)\n        eval_result = [Evaluator.evaluate(m, y_true=y, y_pred=yhat, multioutput=multioutput)\n                       for m in metrics]\n        return eval_result\n\n    def predict(self, x, mc=False, batch_size=32):\n        # reshape 1dim input\n        x = self._reshape_input(x)\n\n        if not self.model_built:\n            raise RuntimeError(\"You must call fit_eval or restore first before calling predict!\")\n        x = PytorchBaseModel.to_torch(x).float()\n        if mc:\n            self.model.train()\n        else:\n            self.model.eval()\n        test_loader = DataLoader(TensorDataset(x),\n                                 batch_size=int(batch_size))\n        yhat_list = []\n        for x_test_batch in test_loader:\n            yhat_list.append(self.model(x_test_batch[0]).detach().numpy())\n        yhat = np.concatenate(yhat_list, axis=0)\n        return yhat\n\n    def predict_with_uncertainty(self, x, n_iter=100):\n        result = np.zeros((n_iter,) + (x.shape[0], self.config[\"output_feature_num\"]))\n        for i in range(n_iter):\n            result[i, :, :] = self.predict(x, mc=True)\n\n        prediction = result.mean(axis=0)\n        uncertainty = result.std(axis=0)\n        return prediction, uncertainty\n\n    def state_dict(self):\n        state = {\n            \"config\": self.config,\n            \"model\": self.model.state_dict(),\n            \"optimizer\": self.optimizer.state_dict(),\n        }\n        return state\n\n    def load_state_dict(self, state):\n        self.config = state[\"config\"]\n        self.model = self.model_creator(self.config)\n        self.model.load_state_dict(state[\"model\"])\n        self.model_built = True\n        self._create_optimizer()\n        self.optimizer.load_state_dict(state[\"optimizer\"])\n        self._create_loss()\n\n    def save(self, checkpoint):\n        if not self.model_built:\n            raise RuntimeError(\"You must call fit_eval or restore first before calling save!\")\n        state_dict = self.state_dict()\n        torch.save(state_dict, checkpoint)\n\n    def restore(self, checkpoint):\n        state_dict = torch.load(checkpoint)\n        self.load_state_dict(state_dict)\n\n    def evaluate_with_onnx(self, x, y, metrics=['mse'], dirname=None, multioutput=\"raw_values\"):\n        # reshape 1dim input\n        x = self._reshape_input(x)\n        y = self._reshape_input(y)\n\n        yhat = self.predict_with_onnx(x, dirname=dirname)\n        eval_result = [Evaluator.evaluate(m, y_true=y, y_pred=yhat, multioutput=multioutput)\n                       for m in metrics]\n        return eval_result\n\n    def _build_onnx(self, x, dirname=None):\n        if not self.model_built:\n            raise RuntimeError(\"You must call fit_eval or restore\\\n                               first before calling onnx methods!\")\n        try:\n            import onnx\n            import onnxruntime\n        except:\n            raise RuntimeError(\"You should install onnx and onnxruntime to use onnx based method.\")\n        if dirname is None:\n            dirname = tempfile.mkdtemp(prefix=\"onnx_cache_\")\n        # code adapted from\n        # https:\/\/pytorch.org\/tutorials\/advanced\/super_resolution_with_onnxruntime.html\n        torch.onnx.export(self.model,\n                          x,\n                          os.path.join(dirname, \"cache.onnx\"),\n                          export_params=True,\n                          opset_version=10,\n                          do_constant_folding=True,\n                          input_names=['input'],\n                          output_names=['output'],\n                          dynamic_axes={'input': {0: 'batch_size'},\n                                        'output': {0: 'batch_size'}})\n        self.onnx_model = onnx.load(os.path.join(dirname, \"cache.onnx\"))\n        onnx.checker.check_model(self.onnx_model)\n        self.ort_session = onnxruntime.InferenceSession(os.path.join(dirname, \"cache.onnx\"))\n        self.onnx_model_built = True\n\n    def predict_with_onnx(self, x, mc=False, dirname=None):\n        # reshape 1dim input\n        x = self._reshape_input(x)\n\n        x = PytorchBaseModel.to_torch(x).float()\n        if not self.onnx_model_built:\n            self._build_onnx(x[0:1], dirname=dirname)\n\n        def to_numpy(tensor):\n            return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n        ort_inputs = {self.ort_session.get_inputs()[0].name: to_numpy(x)}\n        ort_outs = self.ort_session.run(None, ort_inputs)\n        return ort_outs[0]\n\n    def _get_required_parameters(self):\n        return {}\n\n    def _get_optional_parameters(self):\n        return {\"batch_size\",\n                LR_NAME,\n                \"dropout\",\n                \"optim\",\n                \"loss\"\n                }\n\n\nclass PytorchModelBuilder(ModelBuilder):\n\n    def __init__(self, model_creator,\n                 optimizer_creator,\n                 loss_creator):\n        from zoo.orca.automl.pytorch_utils import validate_pytorch_loss, validate_pytorch_optim\n        self.model_creator = model_creator\n        optimizer = validate_pytorch_optim(optimizer_creator)\n        self.optimizer_creator = optimizer\n        loss = validate_pytorch_loss(loss_creator)\n        self.loss_creator = loss\n\n    def build(self, config):\n        model = PytorchBaseModel(self.model_creator,\n                                 self.optimizer_creator,\n                                 self.loss_creator)\n        model.build(config)\n        return model\n"},"license":{"2":"bsd-3-clause","3":"apache-2.0","4":"apache-2.0"},"hash":{"2":8631834601338594377,"3":-3023533144631287161,"4":-3865462158614233531},"line_mean":{"2":34.0106382979,"3":34.7068584071,"4":38.890052356},"line_max":{"2":120,"3":99,"4":99},"alpha_frac":{"2":0.6797326041,"3":0.5116329502,"4":0.5790786192},"autogenerated":{"2":false,"3":false,"4":false}}